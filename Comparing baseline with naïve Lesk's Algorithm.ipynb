{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and apply Lesk’s algorithm to the publicly available data set of SemEval 2013 Shared Task \\#12 \\(Navigli and Jurgens, 2013\\), using NLTK’s interface to WordNet v3.0 as your lexical resource.\n",
    "(Be sure you are using WordNet v3.0!) The relevant files are available on the course website. Starter\n",
    "code is also provided to help you load the data. More information on the data set can be found at\n",
    "https://www.cs.york.ac.uk/semeval-2013/task12/.\n",
    "The provided code will load all of the cases that you are to resolve, along with their sentential context.\n",
    "Apply word tokenization and lemmatization (you have code to do this from A1) as necessary, and remove\n",
    "stop words.\n",
    "As a first step, compare the following two methods for WSD:\n",
    "\n",
    "1. The most frequent sense baseline: this is the sense indicated as #1 in the synset according to\n",
    "WordNet\n",
    "2. NLTK’s implementation of Lesk’s algorithm (nltk.wsd.lesk)\n",
    "Use accuracy as the evaluation measure. There is sometimes more than one correct sense annotated in\n",
    "the key. If that is the case, you may consider an automatic system correct if it resolves the word to any\n",
    "one of those senses. What do you observe about the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/2013/dgarfi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/2013/dgarfi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def lemmatize_sentence(list_or_str):\n",
    "    if isinstance(list_or_str, str):\n",
    "        sentence_list = list_or_str.split()\n",
    "    else:\n",
    "        sentence_list = list_or_str\n",
    "    return [lemmatizer.lemmatize(word) for word in sentence_list if word not in stopwords]\n",
    "\n",
    "def preprocess_wsdinstance(wsdinstance):\n",
    "    wsdinstance.context = lemmatize_sentence(wsdinstance.context)\n",
    "    return wsdinstance\n",
    "    \n",
    "dev_instances = { k: preprocess_wsdinstance(wsdinstance) for k, wsdinstance in dev_instances.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synsets_from_keys(keys_dict):\n",
    "    \"\"\"\n",
    "    :param keys_dict: a dictionary of word-key: [synset ids] mappings\n",
    "    :return dict: returns the same dictionary but maps [synset ids] to their corresponding Synset() objects\n",
    "    \"\"\"\n",
    "    return { k: [wn.lemma_from_key(key).synset() for key in list_of_keys]\n",
    "               for k, list_of_keys in keys_dict.items()}\n",
    "\n",
    "expected_dev = get_synsets_from_keys(dev_key)\n",
    "expected_test = get_synsets_from_keys(test_key)\n",
    "\n",
    "def accuracy_of(predicted, expected):\n",
    "    \"\"\"\n",
    "    :param predicted dict: word-key: Synset() dictionary of predicted word-sense disambiguations\n",
    "    :param expected dict: word-key: Synset() of expected word-sense disambiguations    \n",
    "    :return float: the percent of total keys in expected which are correctly predicted\n",
    "    \n",
    "    Note that expected.values() maps our word-based lexicon to lists of Synsets.\n",
    "    If any one of its possible Synsets is predicted, the word is considered correcty predicted.\n",
    "    \"\"\"\n",
    "    assert(len(predicted.values()) == len(expected.values()))\n",
    "    return sum(map(lambda x: x[0] in x[1], zip(predicted.values(), expected.values()))) / float(len(predicted.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Baseline and Lesk's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038620689655172416"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_with(method, instance_dict):\n",
    "    return { k: method(wsd) for k, wsd in instance_dict.items() }\n",
    "    \n",
    "def baseline(target_instance):\n",
    "    return wn.synsets(target_instance.lemma)[0]\n",
    "\n",
    "predicted_baseline = predict_with(baseline, test_instances)\n",
    "\n",
    "accuracy_of(predicted_baseline, expected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022758620689655173"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lesk = { k: nltk.wsd.lesk(wsd.context, wsd.lemma) for k, wsd in test_instances.items() }\n",
    "\n",
    "accuracy_of(predicted_lesk, expected_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Combination of Word Sense Frequency & Lesk's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
