{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and apply Lesk’s algorithm to the publicly available data set of SemEval 2013 Shared Task \\#12 \\(Navigli and Jurgens, 2013\\), using NLTK’s interface to WordNet v3.0 as your lexical resource.\n",
    "(Be sure you are using WordNet v3.0!) The relevant files are available on the course website. Starter\n",
    "code is also provided to help you load the data. More information on the data set can be found at\n",
    "https://www.cs.york.ac.uk/semeval-2013/task12/.\n",
    "The provided code will load all of the cases that you are to resolve, along with their sentential context.\n",
    "Apply word tokenization and lemmatization (you have code to do this from A1) as necessary, and remove\n",
    "stop words.\n",
    "As a first step, compare the following two methods for WSD:\n",
    "\n",
    "1. The most frequent sense baseline: this is the sense indicated as #1 in the synset according to\n",
    "WordNet\n",
    "2. NLTK’s implementation of Lesk’s algorithm (nltk.wsd.lesk)\n",
    "Use accuracy as the evaluation measure. There is sometimes more than one correct sense annotated in\n",
    "the key. If that is the case, you may consider an automatic system correct if it resolves the word to any\n",
    "one of those senses. What do you observe about the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run loader.py\n",
    "\n",
    "from tqdm import tqdm # progress bar\n",
    "import functools # LRU cache\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/2013/dgarfi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/2013/dgarfi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add('--')\n",
    "stopwords.add('@card')\n",
    "stopwords.add('``')\n",
    "for c in string.punctuation:\n",
    "    stopwords.add(c)\n",
    "\n",
    "@functools.lru_cache(maxsize=65536)\n",
    "def lemmatize_sentence(sentence):\n",
    "    sentence_list = sentence.split()\n",
    "    return [lemmatizer.lemmatize(word) for word in sentence_list if word not in stopwords]\n",
    "\n",
    "def preprocess_wsdinstance(wsdinstance):\n",
    "    wsdinstance.context = lemmatize_sentence(\" \".join(wsdinstance.context))\n",
    "    return wsdinstance\n",
    "    \n",
    "dev_instances = { k: preprocess_wsdinstance(wsdinstance) for k, wsdinstance in dev_instances.items() }\n",
    "test_instances = { k: preprocess_wsdinstance(wsdinstance) for k, wsdinstance in test_instances.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at our expected data\n",
    "\n",
    "Let's take a look at the distribution of WordNet synset frequencies that occured in our testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d012.s023.t008', [Synset('contribution.n.01')]),\n",
       " ('d010.s020.t003', [Synset('event.n.01')]),\n",
       " ('d006.s029.t001', [Synset('lawyer.n.01')]),\n",
       " ('d008.s003.t006', [Synset('chemical_element.n.01')]),\n",
       " ('d009.s009.t004', [Synset('democracy.n.02')])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_synsets_from_keys(keys_dict):\n",
    "    \"\"\"\n",
    "    :param keys_dict: a dictionary of word-key: [synset ids] mappings\n",
    "    :return dict: returns the same dictionary but maps [synset ids] to their corresponding Synset() objects\n",
    "    \"\"\"\n",
    "    return { k: [wn.lemma_from_key(key).synset() for key in list_of_keys]\n",
    "               for k, list_of_keys in keys_dict.items()}\n",
    "\n",
    "expected_dev = get_synsets_from_keys(dev_key)\n",
    "expected_test = get_synsets_from_keys(test_key)\n",
    "\n",
    "list(expected_test.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'01': 0.6449175824175825,\n",
       " '02': 0.18269230769230768,\n",
       " '03': 0.08173076923076923,\n",
       " '04': 0.04945054945054945,\n",
       " '05': 0.011675824175824176,\n",
       " '06': 0.008241758241758242,\n",
       " '07': 0.0027472527472527475,\n",
       " '08': 0.0027472527472527475,\n",
       " '09': 0.0020604395604395605,\n",
       " '10': 0.0027472527472527475,\n",
       " '11': 0.008928571428571428,\n",
       " '12': 0.0006868131868131869,\n",
       " '20': 0.0006868131868131869,\n",
       " '29': 0.0006868131868131869}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "sense_counter = Counter()\n",
    "\n",
    "for key in expected_test:\n",
    "    doc, sentence, term = key.split('.')\n",
    "    for synset in expected_test[key]:\n",
    "        lemma, pos, sense = synset.name().split('.')\n",
    "        sense_counter.update({sense: 1})\n",
    "{ key: count / sum(sense_counter.values()) for key, count in sense_counter.items() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def juxtapose_predictions(predicted, expected):\n",
    "    assert(len(predicted.values()) == len(expected.values()))\n",
    "    \n",
    "    res = {}\n",
    "    for key in predicted:\n",
    "        lemma = test_instances[key].lemma\n",
    "        context = \" \".join(test_instances[key].context)\n",
    "        if predicted[key] in expected[key]:\n",
    "            res[key] = [\n",
    "                True, \n",
    "                lemma, \n",
    "                context, \n",
    "                \"Expected synset: \" + predicted[key].name(),\n",
    "                \"Expected definition: \" + predicted[key].definition()]\n",
    "        else:\n",
    "            res[key] = [\n",
    "                False, \n",
    "                lemma,\n",
    "                context,\n",
    "                \"Predicted synset: \" + predicted[key].name(), \n",
    "                \"Predicted definition: \" + predicted[key].definition(),\n",
    "                \"Expected synsets: \" + \",\".join([x.name() for x in expected[key]]),\n",
    "                \"Expected definitions: \" + \",\".join([x.definition() for x in expected[key]])]\n",
    "    return res\n",
    "\n",
    "def false_predictions(predicted, expected):\n",
    "    return { k:v for k,v in juxtapose_predictions(predicted, expected).items() if v[0] is False }\n",
    "            \n",
    "def accuracy_of(predicted, expected):\n",
    "    \"\"\"\n",
    "    :param predicted dict: word-key: Synset() dictionary of predicted word-sense disambiguations\n",
    "    :param expected dict: word-key: Synset() of expected word-sense disambiguations    \n",
    "    :return float: the percent of total keys in expected which are correctly predicted\n",
    "    \n",
    "    Note that expected.values() maps our word-based lexicon to lists of Synsets.\n",
    "    If any one of its possible Synsets is predicted, the word is considered correcty predicted.\n",
    "    \"\"\"\n",
    "    assert(len(predicted.values()) == len(expected.values()))\n",
    "    #return sum([x[0] in x[1] for x in zip(predicted.values(), expected.values())]) / float(len(predicted.values()))\n",
    "\n",
    "    good = 0\n",
    "    jux = juxtapose_predictions(predicted, expected)\n",
    "    for key in jux:\n",
    "        if jux[key][0]:\n",
    "            good += 1\n",
    "    return (good / float(len(jux)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Baseline and Lesk's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1450/1450 [00:00<00:00, 20011.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.623448275862069"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_with(method, instance_dict):\n",
    "    return { k: method(wsd) for k, wsd in tqdm(instance_dict.items()) }\n",
    "    \n",
    "def baseline(target_wsdinstance):\n",
    "    return wn.synsets(target_wsdinstance.lemma)[0]\n",
    "\n",
    "predicted_baseline_test = predict_with(baseline, test_instances)\n",
    "\n",
    "accuracy_of(predicted_baseline_test, expected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29586206896551726"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lesk = { k: nltk.wsd.lesk(wsd.context, wsd.lemma) for k, wsd in test_instances.items() }\n",
    "\n",
    "accuracy_of(predicted_lesk, expected_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Combination of Word Sense Frequency & Lesk's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, develop two additional methods to solve this problem. One of them must combine distributional\n",
    "information about the frequency of word senses, and the standard Lesk’s algorithm. The other may be\n",
    "any other method of your design. The two methods must be substantially different; they may not be\n",
    "simply the same method with a different parameter value. Make and justify decisions about any other\n",
    "parameters to the algorithms, such as what exactly to include in the sense and context representations,\n",
    "how to compute overlap, and how to trade off the distributional and the Lesk signal, with the use of\n",
    "the development set, which the starter code will load for you. You may use any heuristic, probabilistic\n",
    "model, or other statistical method that we have discussed in class in order to combine these two sources\n",
    "of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negotiation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9961089494163424"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prior(wsdinstance, synset, power=1):\n",
    "    wordnet_synsets = wn.synsets(wsdinstance.lemma)\n",
    "    \n",
    "    rank = wordnet_synsets[::-1].index(synset) + 1\n",
    "    normalization_const = sum(pow(x, power) for x in range(1, len(wordnet_synsets) + 1))\n",
    "    probability = pow(rank, power) / float(normalization_const)\n",
    "    \n",
    "    return probability\n",
    "     \n",
    "## Debug ##\n",
    "wsd = list(dev_instances.values())[4]\n",
    "print(wsd.lemma)\n",
    "prior(wsd, wn.synsets(wsd.lemma)[0], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Power is pretty bad here\n",
    "def lesks(wsdinstance, target_synset, power=1):\n",
    "    target_index = wn.synsets(wsdinstance.lemma).index(target_synset)\n",
    "    synsets = [set(lemmatize_sentence(synset.definition())) for synset in wn.synsets(wsdinstance.lemma)]\n",
    "    pattern = set(wsdinstance.context)\n",
    "    overlaps = [pow(len(pattern.intersection(synset)), power) for synset in synsets]\n",
    "    \n",
    "    if sum(overlaps) == 0:\n",
    "        return 1 / float(len(wn.synsets(wsdinstance.lemma)))\n",
    "    else:\n",
    "        return overlaps[target_index] / float(sum(overlaps))\n",
    "\n",
    "## Debug ##\n",
    "lesks(wsd, wn.synsets(wsd.lemma)[0], power=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1450/1450 [00:41<00:00, 35.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def posterior_helper(wsdinstance, target_synset):\n",
    "    return prior(wsdinstance, target_synset) * lesks(wsdinstance, target_synset)\n",
    "\n",
    "def posterior(wsdinstance, target_synset):\n",
    "    new_evidences = [posterior_helper(wsdinstance, synset) for synset in wn.synsets(wsdinstance.lemma)]\n",
    "    \n",
    "    return posterior_helper(wsdinstance, target_synset) / float(sum(new_evidences))\n",
    "\n",
    "def bayesian(wsdinstance):\n",
    "    probabilities = [posterior(wsdinstance, synset) for synset in wn.synsets(wsdinstance.lemma)]\n",
    "    return wn.synsets(wsdinstance.lemma)[probabilities.index(max(probabilities))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1450/1450 [00:28<00:00, 50.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48413793103448277"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_bayesian_test = predict_with(bayesian, test_instances)\n",
    "\n",
    "accuracy_of(predicted_bayesian_test, expected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
