{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and apply Lesk’s algorithm to the publicly available data set of SemEval 2013 Shared Task \\#12 \\(Navigli and Jurgens, 2013\\), using NLTK’s interface to WordNet v3.0 as your lexical resource.\n",
    "(Be sure you are using WordNet v3.0!) The relevant files are available on the course website. Starter\n",
    "code is also provided to help you load the data. More information on the data set can be found at\n",
    "https://www.cs.york.ac.uk/semeval-2013/task12/.\n",
    "The provided code will load all of the cases that you are to resolve, along with their sentential context.\n",
    "Apply word tokenization and lemmatization (you have code to do this from A1) as necessary, and remove\n",
    "stop words.\n",
    "As a first step, compare the following two methods for WSD:\n",
    "\n",
    "1. The most frequent sense baseline: this is the sense indicated as #1 in the synset according to\n",
    "WordNet\n",
    "2. NLTK’s implementation of Lesk’s algorithm (nltk.wsd.lesk)\n",
    "Use accuracy as the evaluation measure. There is sometimes more than one correct sense annotated in\n",
    "the key. If that is the case, you may consider an automatic system correct if it resolves the word to any\n",
    "one of those senses. What do you observe about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/2013/dgarfi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d001.s001.t002\tgroup\tU.N. group draft plan to reduce emission\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbg_id = 'd001.s001.t002'\n",
    "dbg = dev_instances[dbg_id]\n",
    "print(str(dbg))\n",
    "list(filter(lambda x: not x.startswith(\"__\"), dir(dbg)))\n",
    "sum([True, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synsets_from_keys(keys_dict):\n",
    "    \"\"\"\n",
    "    :param keys_dict: a dictionary of word-key: [synset ids] mappings\n",
    "    :return dict: returns the same dictionary but maps [synset ids] to their corresponding Synset() objects\n",
    "    \"\"\"\n",
    "    return { k: [wn.lemma_from_key(key).synset() for key in list_of_keys]\n",
    "               for k, list_of_keys in keys_dict.items()}\n",
    "\n",
    "expected_dev = get_synsets_from_keys(dev_key)\n",
    "expected_test = get_synsets_from_keys(test_key)\n",
    "\n",
    "def accuracy_of(predicted, expected):\n",
    "    \"\"\"\n",
    "    :param predicted dict: word-key: Synset() dictionary of predicted word-sense disambiguations\n",
    "    :param expected dict: word-key: Synset() of expected word-sense disambiguations    \n",
    "    :return float: the percent of total keys in expected which are correctly predicted\n",
    "    \n",
    "    Note that expected.values() maps our word-based lexicon to lists of Synsets.\n",
    "    If any one of its possible Synsets is predicted, the word is considered correcty predicted.\n",
    "    \"\"\"\n",
    "    assert(len(predicted.values()) == len(expected.values()))\n",
    "    return sum(map(lambda x: x[0] in x[1], zip(predicted.values(), expected.values()))) / float(len(predicted.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(target_instance):\n",
    "    return wn.synsets(target_instance.lemma)[0]\n",
    "\n",
    "predicted_baseline = { k: baseline(wsd) for k, wsd in dev_instances.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.061855670103092786"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_of(predicted_baseline, expected_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetCorpusReader' object has no attribute 'Synset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-4f53b55af495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSynset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'WordNetCorpusReader' object has no attribute 'Synset'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
